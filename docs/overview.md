# 7aigent

7aigent is an general autonomous AI agent. It runs an interaction loop where an LLM executes actions in a system, and then receives feedback as a result of these actions, in order to complete a given task.

# Interaction loop

The main interaction loop of 7aigent is as follows:
1. Three special messages: a system, task, and screen message - are initialized.
2. Based on the three special messages, and the conversation history, an LLM generates a new message, containing some thoughts and explanations, and then a command.
3. The command is parsed and executed by the system. It then generates a response message, and also updates the "screen" message.

# LLM

The core of our agent interaction is an LLM. It receives the following messages:
1. System message, containing instructions for the LLM, including:
- General directions the LLM needs to follow; and
- Description of the system and how to interact with it.
2. Task message, explaining the specific task that the agent needs to do. It might also contain examples of how to solve similar tasks.
3. Conversation history: the messages generated by the LLM so far, as well as the environment responses.
- History will be truncated when reaching a configured limit by dropping the oldest messages (simple truncation strategy).
4. Screen message: like a computer screen, this contains elements of the current state of the system that get continuously updated as commands are executed. What's shown on the screen message will depend on the commands executed so far.

In response to these inputs, the LLM will generate a message like this:
````markdown
Thoughts, planning, and explanations of what needs to be done next.
```environment
command
```
````

It contains some generated text, followed by a single command to be executed in the system. This command is enclosed in a markdown code block, where the language marker is used to specify which environment it needs to be executed in (e.g. a bash or a python environment, see [system](#system))

# System

The system consists of two main processes: the **agent** (outside the container) and the **orchestrator** (inside the container), which manages multiple **environments**.

## Architecture

**Project directory**: The agent will have access to a specific directory on the file system. We'll call it the "project directory".

**Agent**: Runs outside the container. Responsible for:
- Managing the interaction loop with the LLM
- Parsing LLM responses to extract commands
- Sending commands to the orchestrator
- Receiving responses and updating the conversation history
- Implementing retry logic and error handling for LLM communication

**Container**: A sandboxed container (using Podman) which only has access to the project directory and specific (whitelisted) internet resources.

**Orchestrator**: Runs inside the container as the main process. Responsible for:
- Receiving commands from the agent via stdin
- Routing commands to the appropriate environment
- Managing long-running environment processes
- Collecting screen updates from all environments
- Sending responses back to the agent via stdout
- Validating and loading ad-hoc environment modules

**Agent-Orchestrator Communication**: The agent and orchestrator communicate via stdin/stdout:
- Agent writes commands to orchestrator's stdin
- Orchestrator writes responses to stdout
- Protocol: Line-based or length-prefixed messages (to be determined)

**Environments**: These are the core of the system that handle processing the actual commands. We'll have several built-in environments: `bash`, `python`, and `editor`, as well as ad-hoc environments that the agent can create as needed.

## Environments

We'll have multiple environments, each of which processes different kinds of commands.

**Environment Implementation**: Environments are Python modules that implement a defined contract (to be designed). They are loaded dynamically by the orchestrator.

**Environment Lifecycle**: Environments are long-running processes, spawned by the orchestrator and reused across multiple commands.

**Orchestrator-Environment Management**: The orchestrator directly manages environment child processes (no supervisor/init system).

**Screen Updates**: After each command execution, the orchestrator uses a pull model to request screen updates from environments.

Each environment:
- Is stateful (maintains state across commands)
- Has a section on the screen message it can edit, limited in length
- Executes commands, and as a result:
  - produces a response
  - and updates its screen section

### Bash

The main environment for the agent's use, allows running arbitrary non-interactive commands.
* Command: a bash command and its parameters.
* Response: the stdout and stderr outputs of the command.
* Screen: exit code of last command, current directory.
* Example command:
```bash
ls -al
```

### Python

An environment for running python commands.
* Command: python code
* Response: Output of executed code. If the code is an expression, also its result.
* Screen: Current local and global variables, and their values (if they're short)
* Example command:
```python
print("Hello world")
```

### Editor

An environment for viewing and editing files.
* Commands: a set of commands for viewing and editing files (will be defined later)
* Response: Confirmation whether the last command was successful and, if appropriate, AI-generated summary of content.
* Screen: A set of "views": each view shows part of a file, kept up to date.
* Example command:
```editor
goto_definition main
```
* Example view:
```
> hello.c: function "main", lines 4-6
void main() {
  printf("Hello world\n");
}
```

### Ad-hoc environments

The agent can create new environments. Any Python modules in the `env/` subdirectory of the project directory will be loaded by the orchestrator. The orchestrator validates each module at load time using runtime introspection to ensure it implements the required contract. If validation fails, diagnostic messages are displayed on the screen and the module is not loaded.

# Screen

The screen is a special message, meant to show the current state of the environments (or relevant parts of it). Each environment will have a section on the screen. After each message, all sections will be regenerated.

Its use is best illustrated via examples:
1. When the AI wants to read a function, the response to the read command would be a summary of what the function does, but the actual source would go on the screen. The source needs to be updated every time the file changes.
2. The AI wants to view the definition of the `main` function in `main.c`. The editor environment will remember to show the main function (semantically, not by line numbers). After each message, it will re-read the file, and place the current version of the `main` function on screen.
2. The bash environment has small state - limited to only the current directory. So it can place its full state on screen.

